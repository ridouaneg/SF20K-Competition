<style>
  .center {
    text-align: center;
  }
  pre.code {
    background-color: #f4f4f4;
    padding: 10px;
    overflow-x: auto;
    border-radius: 5px;
    font-family: monospace;
  }
</style>

<h4 class="center"><strong>Short-Films 20K (SF20K) Competition</strong></h4>

<p class="center">
  <a href="https://slomo-workshop.github.io/">Workshop</a> |
  <a href="https://ridouaneg.github.io/sf20k.html">Benchmark</a> |
  <a href="https://arxiv.org/abs/2406.10221">arXiv</a> |
  <a href="https://huggingface.co/datasets/rghermi/sf20k">Dataset</a> |
  <a href="https://github.com/ridouaneg/sf20k">Code</a>
</p>

<p>
  The <strong>Short-Films 20K (SF20K) Competition</strong> aims to advance story-level video understanding 
  by leveraging the new SF20K dataset. While recent multimodal models have demonstrated 
  progress in video understanding, existing benchmarks are largely limited to short videos with simple narratives. In contrast, 
  our competition focuses on complex, long-term reasoning in storytelling by introducing multiple-choice and open-ended 
  question answering tasks.
</p>

<p>
  The competition is based on <strong>SF20K-Test-Expert</strong>, a subset of the SF20K dataset, which includes manually crafted open-ended questions.
  The questions are designed to be challenging, requiring long-term reasoning and complex understanding of the video content.
</p>

<h5>Competition Tracks</h5>

<p></p>
  The competition is split into two tracks, both focusing on <strong>Open-Ended Question Answering</strong>:
  <ul>
    <li><strong>Track 1: Open-Ended QA -  Small Model Track (â‰¤ 7B parameters)</strong><br>
      This track is designed for models with a parameter count of 7 billion or less. 
    </li>
    <li><strong>Track 2: Open-Ended QA - Unlimited Model Track (No size limit)</strong><br>
      This track is designed for models with no size limit.
    </li>
  </ul>
</p>

<h5>SF20K Dataset</h5>

<p>
  The competition is powered by SF20K, a large-scale, publicly available dataset of 20,143 short films, 
  amounting to 3,684 hours of video content. Each film is self-contained, lasting 5 to 40 minutes 
  (average 11 minutes), and spans a variety of genres. SF20K includes automatically generated and manually created question-answer (QA) pairs.
</p>

<p>
  <strong>Dataset Splits</strong>
  <ul>
    <li><strong>SF20K-Train:</strong> 19,071 movies with 191,007 automatically generated QA pairs, provided for model training.</li>
    <li><strong>SF20K-Test-Expert (public):</strong> 50 movies with 500 QA pairs, used for validation.</li>
    <li><strong>SF20K-Test-Expert (private):</strong> 50 movies with 500 QA pairs, used for final ranking.</li>
  </ul>
</p>

<p>
  <strong>Accessing the Dataset</strong>
  You can access the dataset on <a href="https://huggingface.co/datasets/rghermi/sf20k">Hugging Face</a>. 
  Supplementary annotations (i.e., shot boundaries, subtitles, and face tracks) are available 
  on <a href="https://github.com/ridouaneg/sf20k">GitHub</a>. The videos themselves are hosted on YouTube 
  and Vimeo, with URLs provided within the dataset.
</p>

<h5>Evaluation</h5>

<p>
  Submissions should provide <strong>free-form answers</strong>, typically short and concise sentences. The evaluation relies 
  on <a href="https://github.com/ridouaneg/sf20k#evaluation-metric">LLM-QA-Eval</a>, an LLM-assisted correctness assessment using Meta-Llama-3-8B-Instruct. 
  This metric compares ground-truth and predicted answers, assigning a score from 1 to 5. The final metric is the average score.
</p>

<h5>Competition Phases</h5>

<p>
  The competition is divided into two phases:
<ul>
  <li><strong>Phase 1: Public Test Set Evaluation </strong><br>
    During this initial phase, methods are evaluated against the public test set. 
    This phase is primarily for validation purposes, allowing participants to refine their approaches.
  </li>
  <li><strong>Phase 2: Private Test Set Evaluation</strong><br>
    The second phase involves evaluation on the <strong>private test set</strong>, 
    which will be released on <strong>October 3rd, 2025</strong>. Final rankings will 
    be determined based on performance in this phase. Further instructions 
    regarding the private test set evaluation will be provided closer to the release date.
  </li>
</ul>        

<h5>Important Dates</h5>

<ul>
    <li><strong>June 19th, 2025:</strong> The competition server launches with data from the public test set.</li>
    <li><strong>October 3rd, 2025:</strong> Submission deadline for leaderboard ranking on the public test set.</li>
    <li><strong>October 3rd - 10th, 2025:</strong> Private test set evaluation.</li>
    <li><strong>October 10th, 2025:</strong> Final rankings announced on the private test set.</li>
    <li><strong>October 19th - 20th, 2025:</strong> Workshop at ICCV 2025 with winner's presentations.</li>
</ul>

<h5>Starter Code</h5>

<p>
  To help you get started, we provide resources for downloading videos and a tutorial notebook:
  <ul>
      <li><strong>Download Videos:</strong> Download the videos by following the instructions <a href="https://github.com/ridouaneg/sf20k#download-videos">here</a>.</li>
      <li><strong>Tutorial Notebook:</strong> Follow the tutorial notebook <a href="https://github.com/ridouaneg/sf20k#tutorial-notebook">here</a>.</li>
  </ul>
</p>

<h5>Citation</h5>

If you use the SF20K dataset or related resources in your research, please cite the following paper:

<pre class="code">
@article{ghermi2024shortfilmdatasetsfd,
  title     = {Long Story Short: Story-level Video Understanding from 20K Short Films},
  author    = {Ridouane Ghermi and Xi Wang and Vicky Kalogeiton and Ivan Laptev},
  journal   = {arXiv preprint arXiv:2406.10221},
  year      = {2024}
}
</pre>
